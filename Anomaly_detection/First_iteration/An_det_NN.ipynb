{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04767544-ba06-46d4-8ba6-c5a8a2c46f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import iisignature\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e9c59-e770-413f-8c8c-a0b6a589189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fd689-9459-4bf4-b7ef-5d7301868833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multidim_ou_process(n_samples, n_steps, mu=0, theta=0.15, sigma=0.3, n_features=1):\n",
    "    dt = 1.0\n",
    "    X = np.zeros((n_samples, n_steps, n_features), dtype=np.float32)\n",
    "    X[:,0,:] = mu\n",
    "    for t in range(1, n_steps):\n",
    "        dX = theta * (mu - X[:, t-1, :]) * dt + sigma * np.random.normal(size=(n_samples, n_features)) * np.sqrt(dt)\n",
    "        X[:, t, :] = X[:, t-1, :] + dX\n",
    "    return X\n",
    "\n",
    "def generate_ou_process(n_samples, n_steps, mu=0, theta=0.15, sigma=0.3, time_feature = False):\n",
    "    dt = 1.0\n",
    "    if time_feature:\n",
    "        X = np.zeros((n_samples, n_steps, 2), dtype=np.float32)\n",
    "        timeline = np.linspace(0, 1, n_steps)\n",
    "        X[:,0,1] = mu\n",
    "\n",
    "    else:\n",
    "        X = np.zeros((n_samples, n_steps, 1), dtype=np.float32)\n",
    "        X[:,0,0] = mu\n",
    "    for t in range(1, n_steps):\n",
    "        dX = theta * (mu - X[:, t-1, 0]) * dt + sigma * np.random.normal(size=n_samples) * np.sqrt(dt)\n",
    "        if time_feature:\n",
    "            X[:, t, 1] = X[:, t-1, 1] + dX\n",
    "            X[:, t, 0] = timeline[t]\n",
    "        else:\n",
    "            X[:, t, 0] = X[:, t-1,0] + dX\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023193c-62d6-40df-a111-2c8070f9e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddTimeline(X):\n",
    "    '''\n",
    "    X: samples, steps, features\n",
    "    '''\n",
    "    samples, steps, features = X.shape\n",
    "    timeline = np.linspace(0, 1, steps)  # shape: (steps,)\n",
    "    timeline = np.tile(timeline, (samples, 1))  # shape: (samples, steps)\n",
    "    timeline = timeline[:, :, np.newaxis]  # shape: (samples, steps, 1)\n",
    "\n",
    "    X_new = np.concatenate((timeline, X), axis=2)  # вставка timeline как первой фичи\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454b8dd-5e27-49cc-aaee-69242046d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_data(X, sig_level) :\n",
    "    (n_samples, n_steps, n_features) = X.shape\n",
    "    if (n_features == 1) :\n",
    "        print('Warning: only 1 feature detected, adding timeline might be needed')\n",
    "    sig_length = iisignature.siglength(n_features, sig_level)\n",
    "    Y = np.zeros((n_samples, sig_length), dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        Y[i] = iisignature.sig(X[i, :, :], sig_level)\n",
    "    return Y\n",
    "\n",
    "def logsig_data(X, sig_level) :\n",
    "    (n_samples, n_steps, n_features) = X.shape\n",
    "    if (n_features == 1) :\n",
    "        print('Warning: only 1 feature detected, adding timeline might be needed')\n",
    "    s = iisignature.prepare(n_features, sig_level)\n",
    "    sig_length = iisignature.logsiglength(n_features + 1, sig_level)\n",
    "    Y = np.zeros((n_samples, sig_length), dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        Y[i] = iisignature.logsig( X[i, :, :], s)\n",
    "    return Y\n",
    "\n",
    "def RandomizedSignatureBatch(X_batch, k, activation='tanh', seed=None):\n",
    "    '''\n",
    "    Compute Randomized Signature with random matrixes A, b shared between samples\n",
    "    X_batch : (samples, timesteps, features)\n",
    "    \n",
    "    activation func: 'linear', 'tanh', 'relu' or callable\n",
    "\n",
    "    seed : random seed\n",
    "\n",
    "    Returns Z_batch : (samples, steps, k)\n",
    "    k-dimensional r-sig at each time step for each sample.\n",
    "    '''\n",
    "    X_batch = np.asarray(X_batch)\n",
    "    samples, n, d = X_batch.shape\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    A = rng.standard_normal(size=(d, k, k))\n",
    "    b = rng.standard_normal(size=(d, k))\n",
    "\n",
    "    Z_batch = np.zeros((samples, n, k))\n",
    "    Z_batch[:, 0, :] = rng.standard_normal(size=(samples, k))\n",
    "\n",
    "    if activation == 'linear':\n",
    "        def sigma(x): return x / np.sqrt(k)\n",
    "    elif activation == 'tanh':\n",
    "        sigma = np.tanh\n",
    "    elif activation == 'relu':\n",
    "        sigma = lambda x: np.maximum(x, 0)\n",
    "    elif callable(activation):\n",
    "        sigma = activation\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation\")\n",
    "\n",
    "    dX_batch = np.diff(X_batch, axis=1)\n",
    "\n",
    "    for j in range(1, n):\n",
    "        z_prev = Z_batch[:, j-1, :]\n",
    "        increment = np.zeros_like(z_prev)\n",
    "        for i in range(d):\n",
    "            projected = (z_prev @ A[i].T) + b[i]\n",
    "            increment += sigma(projected) * dX_batch[:, j-1, i:i+1]\n",
    "        Z_batch[:, j, :] = z_prev + increment\n",
    "\n",
    "    return Z_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b452eb3-8bcb-430b-8467-c4dbf3bd9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen_OU_1_Dataset(train_samples, random_seed) :\n",
    "    np.random.seed(random_seed)\n",
    "    steps = 20\n",
    "    train_1 = generate_ou_process(int(train_samples * 0.5), steps, 0, 0.2, 0.4)\n",
    "    train_2 = generate_ou_process(int(train_samples * 0.5), steps, 0, 0.15, 0.3)\n",
    "    train = np.concatenate([train_1, train_2], axis=0)\n",
    "    train_labels = np.zeros((train_samples)).astype(int)\n",
    "    train_labels[:int(train_samples * 0.5)] = 1\n",
    "    return train, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b123cc2-6342-4569-bea8-2383ecc5d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen_OU_2_Dataset(train_samples, random_seed) :\n",
    "    np.random.seed(random_seed)\n",
    "    steps = 20\n",
    "    features = 10\n",
    "    train_1 = generate_multidim_ou_process(train_samples//2, steps, 0, 0.15, 0.3, features)\n",
    "    for i in range(len(train_1)):\n",
    "        train_1[i, :, i%features] = generate_ou_process(1, steps, 0, 0.3, 0.6).reshape(steps)\n",
    "    train_2 = generate_multidim_ou_process(train_samples//2, steps, 0, 0.15, 0.3, features)\n",
    "    train = np.concatenate([train_1, train_2], axis=0)\n",
    "    train_labels = np.zeros((train_samples)).astype(int)\n",
    "    train_labels[:train_samples//2] = 1\n",
    "    return train, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1b6f3-92ae-4fe3-9da3-71b6726eefb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_layers:\n",
    "            linear = nn.Linear(in_features, hidden_dim)\n",
    "            nn.init.kaiming_normal_(linear.weight, nonlinearity='leaky_relu', a=0.01)\n",
    "            layers.append(linear)\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            layers.append(nn.Dropout(p=dropout_rate))\n",
    "            in_features = hidden_dim\n",
    "\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(in_features, 1)\n",
    "        nn.init.xavier_normal_(self.output.weight, gain = nn.init.calculate_gain('sigmoid'))  # можно использовать Xavier для последнего слоя\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17cade-b1be-4014-b2f1-77c19eb66ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualMetricEarlyStopping:\n",
    "    def __init__(self, patience, min_delta):\n",
    "        \"\"\"\n",
    "        patience: int — сколько эпох ждать перед остановкой\n",
    "        min_delta: float — минимальное относительное улучшение (0.01 = 1%)\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.history = []\n",
    "\n",
    "    def step(self, accuracy, roc_auc):\n",
    "        self.history.append((accuracy, roc_auc))\n",
    "        \n",
    "        if len(self.history) <= self.patience:\n",
    "            return False \n",
    "\n",
    "        recent = self.history[-self.patience-1:]\n",
    "        base_acc, base_auc = recent[0]\n",
    "        improved = False\n",
    "\n",
    "        for acc, auc in recent[1:]:\n",
    "            acc_gain = (acc - base_acc) / max(base_acc, 1e-6)\n",
    "            auc_gain = (auc - base_auc) / max(base_auc, 1e-6)\n",
    "\n",
    "            if acc_gain >= self.min_delta or auc_gain >= self.min_delta:\n",
    "                improved = True\n",
    "                break\n",
    "\n",
    "        return not improved\n",
    "        \n",
    "def train_NN(model_params, train_params, X, y):\n",
    "    num_epochs = train_params[\"num_epochs\"]\n",
    "    lr = train_params[\"lr\"]\n",
    "    batch_size = train_params[\"batch_size\"]\n",
    "    test_size = train_params[\"test_size\"]\n",
    "    weight_decay = train_params[\"weight_decay\"]\n",
    "    patience = train_params[\"patience\"]\n",
    "    min_delta = train_params[\"min_delta\"]\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, stratify=y, random_state=42)\n",
    "\n",
    "    if (train_params[\"standard_scaler\"]) :\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "    output_size = 1\n",
    "    model = NN(input_size,\n",
    "               model_params[\"hidden_layers\"],\n",
    "               model_params[\"dropout_rate\"]\n",
    "              ).to(device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr = lr, weight_decay=weight_decay)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs//20, gamma=0.8)\n",
    "\n",
    "    best_acc = 0\n",
    "    best_acc_epoch = -1\n",
    "    best_roc_auc = 0\n",
    "    best_roc_auc_epoch = -1\n",
    "    best_metrics = {}\n",
    "\n",
    "    early_stopper = DualMetricEarlyStopping(patience, min_delta)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_outputs = []\n",
    "            all_labels = []\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                outputs = model(batch_X)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                all_outputs.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "            all_outputs = np.array(all_outputs).flatten()\n",
    "            all_labels = np.array(all_labels).flatten()\n",
    "\n",
    "            accuracy = (all_labels == (all_outputs > 0.5)).mean()\n",
    "            roc_auc = roc_auc_score(all_labels, all_outputs)\n",
    "\n",
    "            if best_acc < accuracy:\n",
    "                best_acc = accuracy\n",
    "                best_acc_epoch = epoch+1\n",
    "                \n",
    "            if best_roc_auc < roc_auc:\n",
    "                best_roc_auc = roc_auc\n",
    "                best_roc_auc_epoch = epoch+1\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Roc-auc: {100 * roc_auc:.2f}%, Accuracy: {100*accuracy:.2f}, LR:{scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        if early_stopper.step(accuracy, roc_auc):\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    best_metrics = {\n",
    "        \"accuracy\" : best_acc,\n",
    "        \"roc_auc\" : best_roc_auc,\n",
    "        \"best_acc_epoch\" : best_acc_epoch,\n",
    "        \"best_roc_auc_epoch\" : best_roc_auc_epoch\n",
    "    }\n",
    "    return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f9c48f-15e3-453c-aa1b-d8c4d611a951",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = Gen_OU_1_Dataset(50_000, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe38c7-da16-4960-8c58-97397a0b434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = AddTimeline(X)\n",
    "X_sig = sig_data(X_t, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a54fc3-5773-4c01-8281-fc36224fcf6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b500519-a9c2-4809-b821-a684e23f43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"hidden_layers\": [],\n",
    "    \"dropout_rate\" : 0.2\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"num_epochs\": 500,\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 64,\n",
    "    \"test_size\": 0.05,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"standard_scaler\": True,\n",
    "    \"patience\" : 50,\n",
    "    \"min_delta\" : 0.01\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"1 hl\" : [64],\n",
    "    \"2 hl\" : [128, 64],\n",
    "    \"3 hl\" : [256, 128, 64],\n",
    "    \"4 hl\" : [512, 256, 128, 64],\n",
    "    \"5 hl\" : [1024, 512, 256, 128, 64]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edfcc4-7629-4467-862d-6ea23a5a529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.arange(10, 520, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bca58b-1686-4292-b522-087cd0358e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for key, val in models.items():\n",
    "    print(key, val)\n",
    "    model_params_ = copy.deepcopy(model_params)\n",
    "    model_params_[\"hidden_layers\"] = val\n",
    "    metrics[key] = {}\n",
    "    for sig_len in np.arange(10, 520, 20):\n",
    "        print(model_params_)\n",
    "        metrics[key][sig_len] = train_NN(model_params_, train_params, X.reshape(len(X),-1), y)\n",
    "        print(sig_len)\n",
    "        print(metrics[key][sig_len])\n",
    "        with open('testing_var_sig_data/ou1_metrics_sig.pk1', 'wb') as f:\n",
    "            pickle.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575b070c-c95c-4002-b4cb-d5220d0f8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_metrics(metric_dict):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for model_name, data in metric_dict.items():\n",
    "        x = sorted(data.keys())\n",
    "        acc = [data[k]['accuracy'] for k in x]\n",
    "        plt.plot(x, acc, marker='o', label=model_name)\n",
    "    plt.title('Accuracy vs Input Length')\n",
    "    plt.xlabel('Input Length (Features)')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.yticks(np.arange(0.05, 1.05, 0.05))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot ROC AUC\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for model_name, data in metric_dict.items():\n",
    "        x = sorted(data.keys())\n",
    "        roc = [data[k]['roc_auc'] for k in x]\n",
    "        plt.plot(x, roc, marker='s', label=model_name)\n",
    "    plt.title('ROC AUC vs Input Length')\n",
    "    plt.xlabel('Input Length (Features)')\n",
    "    plt.ylabel('ROC AUC')\n",
    "    plt.ylim(0,1)\n",
    "    plt.yticks(np.arange(0.05, 1.05, 0.05))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef1ff4-02ee-4650-aef0-497425c2d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd983c6-142d-440a-bc99-c0c63b9d5ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-2]",
   "language": "python",
   "name": "conda-env-.conda-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
