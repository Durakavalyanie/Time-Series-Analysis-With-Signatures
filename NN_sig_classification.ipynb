{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcce717-75a0-4534-9e72-3b104d800448",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import iisignature\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb9a376-88ad-4fba-8a37-78bf58173820",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d0e40-9b72-4912-a430-1ff50b7a56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate, num_classes):\n",
    "        \"\"\"\n",
    "        input_dim: int — размер входного вектора\n",
    "        hidden_layers: list[int] — размеры скрытых слоёв, например [128, 64]\n",
    "        dropout_rate: float — dropout rate\n",
    "        num_classes: int — количество классов (>= 2)\n",
    "        \"\"\"\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_layers:\n",
    "            linear = nn.Linear(in_features, hidden_dim)\n",
    "            nn.init.kaiming_normal_(linear.weight, nonlinearity='leaky_relu', a=0.01)\n",
    "            layers.append(linear)\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            layers.append(nn.Dropout(p=dropout_rate))\n",
    "            in_features = hidden_dim\n",
    "\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(in_features, num_classes)\n",
    "        nn.init.xavier_normal_(self.output.weight, gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388e350-edfd-4d9d-a329-0a8ffa74a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience: int, min_delta: float):\n",
    "        \"\"\"\n",
    "        patience — сколько эпох ждать перед остановкой\n",
    "        min_delta — минимальное относительное улучшение (например, 0.01 = 1%)\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.history = []\n",
    "\n",
    "    def step(self, accuracy):\n",
    "        self.history.append(accuracy)\n",
    "        \n",
    "        if len(self.history) <= self.patience:\n",
    "            return False \n",
    "\n",
    "        recent = self.history[-self.patience-1:]\n",
    "        base_acc = recent[0]\n",
    "        improved = False\n",
    "\n",
    "        for acc in recent[1:]:\n",
    "            acc_gain = (acc - base_acc) / max(base_acc, 1e-6)\n",
    "\n",
    "            if acc_gain >= self.min_delta:\n",
    "                improved = True\n",
    "                break\n",
    "\n",
    "        return not improved\n",
    "\n",
    "def grid_search_kfold_and_finetune(X, y, model_params_base, train_params, hidden_layer_grid, sig_len_grid):\n",
    "    '''\n",
    "    Тренировка и k-fold валидация по гиперпараметрам архитектуры NN и длины сигнатуры\n",
    "    С дальнейщим дообучением лучшей модели на всём датасете\n",
    "    '''\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    folds = train_params[\"folds\"]\n",
    "    num_epochs = train_params[\"num_epochs\"]\n",
    "    lr = train_params[\"lr\"]\n",
    "    batch_size = train_params[\"batch_size\"]\n",
    "    weight_decay = train_params[\"weight_decay\"]\n",
    "    patience = train_params[\"patience\"]\n",
    "    min_delta = train_params[\"min_delta\"]\n",
    "    num_classes = train_params[\"num_classes\"]\n",
    "    finetune_epochs = train_params[\"finetune_epochs\"]\n",
    "\n",
    "    best_mean_acc = 0\n",
    "    best_params = None\n",
    "    best_model_state = None\n",
    "    best_scaler = None\n",
    "\n",
    "    for sig_len in sig_len_grid:\n",
    "        X_subset = X[:, :sig_len]\n",
    "\n",
    "        for hidden_layers in hidden_layer_grid:\n",
    "            acc_list = []\n",
    "            skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X_subset, y)):\n",
    "                X_train_fold, X_val_fold = X_subset[train_idx], X_subset[val_idx]\n",
    "                y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "                X_val_scaled = scaler.transform(X_val_fold)\n",
    "\n",
    "                X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "                y_train_tensor = torch.tensor(y_train_fold, dtype=torch.long).to(device)\n",
    "                X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
    "                y_val_tensor = torch.tensor(y_val_fold, dtype=torch.long).to(device)\n",
    "\n",
    "                train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "                val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                model = NN(sig_len, hidden_layers, model_params_base[\"dropout_rate\"], num_classes).to(device)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "                scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=max(1, num_epochs // 20), gamma=0.8)\n",
    "                early_stopper = EarlyStopper(patience, min_delta)\n",
    "\n",
    "                best_fold_acc = 0\n",
    "                best_state = None\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    for batch_X, batch_y in train_loader:\n",
    "                        outputs = model(batch_X)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    scheduler.step()\n",
    "\n",
    "                    model.eval()\n",
    "                    all_outputs = []\n",
    "                    all_labels = []\n",
    "                    with torch.no_grad():\n",
    "                        for batch_X, batch_y in val_loader:\n",
    "                            outputs = model(batch_X)\n",
    "                            preds = outputs.argmax(dim=1)\n",
    "                            all_outputs.extend(preds.cpu().numpy())\n",
    "                            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "                    acc = accuracy_score(all_labels, all_outputs)\n",
    "\n",
    "                    if acc > best_fold_acc:\n",
    "                        best_fold_acc = acc\n",
    "                        best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                    if early_stopper.step(acc):\n",
    "                        break\n",
    "\n",
    "                acc_list.append(best_fold_acc)\n",
    "\n",
    "            mean_acc = np.mean(acc_list)\n",
    "            print(f\"sig_len={sig_len}, hidden_layers={hidden_layers} → mean_acc={mean_acc:.4f}\")\n",
    "\n",
    "            if mean_acc > best_mean_acc:\n",
    "                best_mean_acc = mean_acc\n",
    "                best_params = {\n",
    "                    \"sig_len\": sig_len,\n",
    "                    \"hidden_layers\": hidden_layers\n",
    "                }\n",
    "                best_model_state = best_state\n",
    "                best_scaler = copy.deepcopy(scaler)\n",
    "    print(\"\\nFine-tuning best model on full dataset...\")\n",
    "\n",
    "    # Дообучение на полном датасете\n",
    "    X_full_subset = X[:, :best_params[\"sig_len\"]]\n",
    "    X_full_scaled = best_scaler.transform(X_full_subset)\n",
    "    X_full_tensor = torch.tensor(X_full_scaled, dtype=torch.float32).to(device)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "    full_loader = DataLoader(TensorDataset(X_full_tensor, y_tensor), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Модель с лучшими параметрами и загружаем веса\n",
    "    final_model = NN(best_params[\"sig_len\"], best_params[\"hidden_layers\"], model_params_base[\"dropout_rate\"], num_classes).to(device)\n",
    "    final_model.load_state_dict(best_model_state)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(final_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    final_model.train()\n",
    "    for epoch in range(finetune_epochs):\n",
    "        for batch_X, batch_y in full_loader:\n",
    "            outputs = final_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return best_params, best_mean_acc, final_model, best_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf34cfa-0325-454a-b66d-ef9c524e1015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_bootstrap(model, X_test, y_test, n):\n",
    "    \"\"\"\n",
    "    Оценка модели на n бустрап. выборках\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    outputs = model(X_test)  # логиты\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    acc_on_original_test = accuracy_score(y_test.cpu(), preds.cpu())\n",
    "    \n",
    "    N = len(X_test)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            indices = np.random.choice(N, N, replace=True)\n",
    "            X_sample = X_test[indices]\n",
    "            y_sample = y_test[indices]\n",
    "\n",
    "            outputs = model(X_sample)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            acc = accuracy_score(y_sample.cpu(), preds.cpu())\n",
    "            accuracies.append(acc)\n",
    "    \n",
    "    mean_acc = np.mean(accuracies)\n",
    "    var_acc = np.var(accuracies, ddof=1)\n",
    "\n",
    "    return round(mean_acc * 100, 1), round(var_acc*100, 1), round(acc_on_original_test*100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a094d1b-9f23-4ec7-816c-c6667b4f2ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TEST_TS_LENGTH_EQUALITY(X: pd.DataFrame) -> bool:\n",
    "    lengths = X.map(len)\n",
    "    all_equal = (lengths == lengths.iloc[0,0]).all().all()\n",
    "    return all_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb4203-e294-41a8-8053-0ff35b414381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sktime_df_to_numpy(X):\n",
    "    \"\"\"\n",
    "    Преобразует DataFrame из sktime с Series внутри в numpy-массив (samples, timesteps, features)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_timestamps = len(X.iloc[0, 0])\n",
    "\n",
    "    data = np.zeros((n_samples, n_timestamps, n_features), dtype=np.float32)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_features):\n",
    "            data[i, :, j] = X.iat[i, j].values\n",
    "\n",
    "    return data\n",
    "\n",
    "def sktime_df_to_variable_length_list(X):\n",
    "    \"\"\"\n",
    "    Преобразует DataFrame из sktime с Series внутри в список numpy-массивов переменной длины.\n",
    "    Возвращает: list из массивов формы (seq_len_i, n_features)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    output = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        series_list = []\n",
    "        for j in range(n_features):\n",
    "            series = X.iat[i, j]\n",
    "            series_array = series.to_numpy(dtype=np.float32)\n",
    "            series_list.append(series_array)\n",
    "\n",
    "        sample_array = np.stack(series_list, axis=-1)  # (seq_len, n_features)\n",
    "        output.append(sample_array)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8567aac-6ec3-4ec7-a0e8-f697136112bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddTimeline(X):\n",
    "    '''\n",
    "    X: samples, steps, features\n",
    "    '''\n",
    "    samples, steps, features = X.shape\n",
    "    timeline = np.linspace(0, 1, steps)  # shape: (steps,)\n",
    "    timeline = np.tile(timeline, (samples, 1))  # shape: (samples, steps)\n",
    "    timeline = timeline[:, :, np.newaxis]  # shape: (samples, steps, 1)\n",
    "\n",
    "    X_new = np.concatenate((timeline, X), axis=2)  # вставка timeline как первой фичи\n",
    "    return X_new\n",
    "\n",
    "def AddTimeline_variable(X_list):\n",
    "    \"\"\"\n",
    "        X_list: list of np.ndarray форма (seq_len_i, n_features)\n",
    "\n",
    "    Return:\n",
    "        list of np.ndarray форма (seq_len_i, n_features + 1)\n",
    "    \"\"\"\n",
    "    X_new_list = []\n",
    "\n",
    "    for x in X_list:\n",
    "        seq_len = x.shape[0]\n",
    "        timeline = np.linspace(0, 1, seq_len).reshape(-1, 1)  # (seq_len, 1)\n",
    "        x_with_time = np.concatenate((timeline, x), axis=1)  # (seq_len, n_features + 1)\n",
    "        X_new_list.append(x_with_time)\n",
    "\n",
    "    return X_new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af13ec-23c5-49e8-87e2-f78cb1faa01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_data(X, sig_level) :\n",
    "    (n_samples, n_steps, n_features) = X.shape\n",
    "    if (n_features == 1) :\n",
    "        print('Warning: only 1 feature detected, adding timeline might be needed')\n",
    "    sig_length = iisignature.siglength(n_features, sig_level)\n",
    "    Y = np.zeros((n_samples, sig_length), dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        Y[i] = iisignature.sig(X[i, :, :], sig_level)\n",
    "    return Y\n",
    "\n",
    "def sig_variable_length_data(X, sig_level) :\n",
    "    n_features = X[0].shape[1]\n",
    "    n_samples = len(X)\n",
    "    if (n_features == 1) :\n",
    "        print('Warning: only 1 feature detected, adding timeline might be needed')\n",
    "    sig_length = iisignature.siglength(n_features, sig_level)\n",
    "    Y = np.zeros((n_samples, sig_length), dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        Y[i] = iisignature.sig(X[i], sig_level)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4b30d9-072d-475e-977b-ee135c856bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y_train : pd.Series, y_test : pd.Series):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "\n",
    "    # Проверка на неизвестные классы в y_test\n",
    "    unknown = set(y_test) - set(le.classes_)\n",
    "    if unknown:\n",
    "        raise ValueError(f\"Неизвестные классы в y_test: {unknown}\")\n",
    "\n",
    "    y_train_encoded = le.transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    return np.array(y_train_encoded, dtype=int), np.array(y_test_encoded, dtype=int), le.classes_ #список уник. классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a71ea-9328-46e3-8c30-7632cdea5526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_csv(filename, dataset_name, acc_on_original_test, acc, var, best_params):\n",
    "    '''\n",
    "    Запись результатов в csv с перезатиранием результатов на датасете, если он уже есть в файле\n",
    "    '''\n",
    "    acc_var_str = f\"{acc:.1f} ± {var:.1f}\"\n",
    "\n",
    "    params_items = []\n",
    "    for k, v in best_params.items():\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            v = '-'.join(str(x) for x in v)\n",
    "        params_items.append(str(v))\n",
    "\n",
    "    headers = [\"Dataset\", \"Accuracy on orig test\", \"Mean Accuracy ± Variance\"] + list(best_params.keys())\n",
    "    new_row = [dataset_name, f\"{acc_on_original_test:.1f}\", acc_var_str] + params_items\n",
    "\n",
    "    file_exists = os.path.exists(filename)\n",
    "    rows = []\n",
    "\n",
    "    if file_exists:\n",
    "        with open(filename, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "\n",
    "    if not rows:\n",
    "        rows.append(headers)\n",
    "\n",
    "    # ПЕРЕЗАПИСЫВАЕМ СТРОКУ ЕСЛИ ДАТАСЕТ УЖЕ ЕСТЬ В ФАЙЛЕ\n",
    "    dataset_found = False\n",
    "    for i, row in enumerate(rows[1:], start=1):\n",
    "        if row[0] == dataset_name:\n",
    "            rows[i] = new_row\n",
    "            dataset_found = True\n",
    "            break\n",
    "\n",
    "    if not dataset_found:\n",
    "        rows.append(new_row)\n",
    "\n",
    "    \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e18eb-1a33-4e63-b927-61b4fa9a29f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATASETS_FOLDER = \"/PATH/TO/DATASETS/FOLDER\"\n",
    "DATASETS = ['EthanolConcentration', 'FaceDetection', 'Handwriting', 'Heartbeat',  'JapaneseVowels', 'PEMS-SF', 'SelfRegulationSCP1', 'SelfRegulationSCP2', 'SpokenArabicDigits', 'UWaveGestureLibrary']\n",
    "DATASET_NAME = 'Heartbeat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dd41f-123c-4c53-a5da-b3fe5f82709d",
   "metadata": {},
   "source": [
    "Загружаем данные из .ts в DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2bcd9-f6e4-4885-bdb7-b10c7f88abe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_df, y_train_df = load_from_tsfile_to_dataframe(f'{PATH_TO_DATASETS_FOLDER}/{DATASET_NAME}/{DATASET_NAME}_TRAIN.ts')\n",
    "X_test_df, y_test_df = load_from_tsfile_to_dataframe(f'{PATH_TO_DATASETS_FOLDER}/{DATASET_NAME}/{DATASET_NAME}_TEST.ts')\n",
    "\n",
    "TRAIN_LENGTHS_EQUAL = TEST_TS_LENGTH_EQUALITY(X_train_df)\n",
    "TEST_LENGTHS_EQUAL = TEST_TS_LENGTH_EQUALITY(X_test_df)\n",
    "print(f\"Train dataset has all lengths equal: {TRAIN_LENGTHS_EQUAL}\\nTest dataset has all lengths equal: {TEST_LENGTHS_EQUAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3565875-a013-444a-a1c4-d7b23d5acbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbffb93-bcd3-4ef3-ad8a-4e36f4d0b99a",
   "metadata": {},
   "source": [
    "Переводим в numpy массивы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c204d2d-d9ac-4459-9166-4a830172ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENGTHS_EQUAL:\n",
    "    X_train_np = sktime_df_to_numpy(X_train_df)\n",
    "else:\n",
    "    X_train_np = sktime_df_to_variable_length_list(X_train_df)\n",
    "    \n",
    "if TEST_LENGTHS_EQUAL:\n",
    "    X_test_np = sktime_df_to_numpy(X_test_df)\n",
    "else:\n",
    "    X_test_np = sktime_df_to_variable_length_list(X_test_df)\n",
    "    \n",
    "y_train_np, y_test_np, classes = encode_labels(y_train_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1befbc5-afbb-4103-bde9-e0784507c033",
   "metadata": {},
   "source": [
    "Добавляем фичу времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6809b9b-92c4-4258-8629-27b8e18a3de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENGTHS_EQUAL:\n",
    "    X_train_np_t = AddTimeline(X_train_np)\n",
    "else:\n",
    "    X_train_np_t = AddTimeline_variable(X_train_np)\n",
    "\n",
    "if TEST_LENGTHS_EQUAL:\n",
    "    X_test_np_t = AddTimeline(X_test_np)\n",
    "else:\n",
    "    X_test_np_t = AddTimeline_variable(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dd1a6-618d-4a46-b030-4b57029cdb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(X_train_np_t)\n",
    "train_features_amount = X_train_np_t[0].shape[1]\n",
    "test_size = len(X_test_np_t)\n",
    "test_features_amount = X_test_np_t[0].shape[1]\n",
    "\n",
    "print(f\"Размер train: {train_size}, итоговое количество фичей:{train_features_amount}\")\n",
    "print(f\"Размер test: {test_size}, итоговое количество фичей:{test_features_amount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a027b-38f6-4954-9f6f-2adcecc93e87",
   "metadata": {},
   "source": [
    "Выбор уровня сигнатуры (первый аргумент - количество фичей, вторая - уровень)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23573748-b3d5-43af-b5b6-5490b7e850d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIG_LEVEL = 2\n",
    "print(\"Длина сигнатуры:\", iisignature.siglength(train_features_amount, SIG_LEVEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967c1dd-c78c-49b3-9266-99787291a413",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENGTHS_EQUAL:\n",
    "    X_train_sig = sig_data(X_train_np_t, SIG_LEVEL)\n",
    "else:\n",
    "    X_train_sig = sig_variable_length_data(X_train_np_t, SIG_LEVEL)\n",
    "\n",
    "if TEST_LENGTHS_EQUAL: \n",
    "    X_test_sig = sig_data(X_test_np_t, SIG_LEVEL)\n",
    "else:\n",
    "    X_test_sig = sig_variable_length_data(X_test_np_t, SIG_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e71af41-6793-41c9-b759-024b124172b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Итоговая форма train:\", X_train_sig.shape)\n",
    "print(\"Итоговая форма test:\", X_test_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0edd8f-26fc-44ab-840a-5d8f838b94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"dropout_rate\" : 0.2\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"num_epochs\": 500,        #Влияет на скорость уменьшения learning rate см. grid_search_kfold_and_finetune()\n",
    "    \"num_classes\" : len(classes),\n",
    "    \"folds\" : 5,\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"patience\" : 50,         #Сколько эпох ждёт улучшения accuracy на min_delta EarlyStopper\n",
    "    \"min_delta\" : 0.01,\n",
    "    \"finetune_epochs\" : 10   #Дообучение на всём датасете после валидации\n",
    "}\n",
    "\n",
    "#ВСЕГДА К ДАННЫМ ПРИМЕНЯЕТСЯ StandardScaler()\n",
    "\n",
    "hidden_layer_grid = [ [128, 64], [256, 128, 64], [512, 256, 128, 64], [1024, 512, 256, 128, 64] ]       #Набор гиперпараметров\n",
    "\n",
    "full_sig_length = X_train_sig.shape[1]\n",
    "sig_len_grid = [full_sig_length, full_sig_length // 2, full_sig_length // 4, full_sig_length // 8]      #Набор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314d4bf-a10d-431a-a9b5-c969de7e05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_amount = 100\n",
    "best_params, best_mean_acc, final_model, best_scaler = grid_search_kfold_and_finetune(X_train_sig, y_train_np,\n",
    "                                                     model_params_base = model_params, train_params=train_params,   \n",
    "                                                     hidden_layer_grid = hidden_layer_grid,\n",
    "                                                     sig_len_grid = sig_len_grid)\n",
    "\n",
    "X_test = best_scaler.transform(X_test_sig[:, :best_params['sig_len']])\n",
    "mean_acc_on_bootstrap, var_on_bootstrap, acc_on_original_test = evaluate_with_bootstrap(final_model,  X_test, y_test_np, bootstrap_amount)\n",
    "\n",
    "print(\"Лучшие параметры:\", best_params, \"Mean accuracy on validation:\", best_mean_acc)\n",
    "print(\"Accuracy на тесте:\", acc_on_original_test)\n",
    "print(f\"Mean accuracy +- variance on bootstrap: {mean_acc_on_bootstrap} +- {var_on_bootstrap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb586d4-f03b-4688-9403-b8e8542ce73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSV = \"/PATH/TO/CSV.csv\"\n",
    "\n",
    "write_results_to_csv(PATH_TO_CSV, DATASET_NAME, acc_on_original_test, mean_acc_on_bootstrap, var_on_bootstrap, best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-2]",
   "language": "python",
   "name": "conda-env-.conda-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
